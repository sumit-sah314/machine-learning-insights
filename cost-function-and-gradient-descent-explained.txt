**COST FUNCTION DERIVATION**

So far this summer, I have been deep-diving into Machine Learning-not just using the libraries for business solutions, but actually to understand the math lying behind the algorithms. My journey has been a bit random, and I haven‚Äôt always been consistent, but every now and then I stumble upon something that makes me stop and think. üí°

One of those questions? ‚Å§‚Å§Why do we square errors in the cost function(1/2m‚àë(y‚Ä≤‚àíy)^2)? ‚Å§‚Å§And why do we divide by 2m? ü§∑‚Äç‚ôÇÔ∏è

Was this formula derived mathematically or developed out of intuition? It's a little of both. Here is how a mathematician may have thought through it:

Well, the first question to try and answer is: "How can we measure how far a model's prediction is off from true value?

A mathematician may start by simply comparing predicted (y‚Äô) and observed (y). It is natural to consider their difference y‚Äô‚àíy as a measure of the extent to which a prediction diverges from reality. Summing these errors would just cancel out positive with negative errors and therefore these ones may give rise to misleading conclusions.

One possible solution is to take moduli |y‚Äô‚àíy| of differences because thus all errors will contribute positively. Yet defining an error function has its main intention in minimizing errors and optimizing predictions (with the help of calculus). However since absolute values can‚Äôt be differentiated at zero, optimization gets complex.

That's why the choice of squaring the errors, (y'-y)^2 turns out to be a natural one. Somebody might say "What about cubing or even using higher powers?" The reason is because cubing-or any other odd power-brings back the problem of negative values, whereas raising to even higher powers would make things more complicated than it should be. When one squares, the advantages are that it makes all errors become positive, it gives a smooth differentiable curve everywhere, and leads to absolute minimum. 

Next, mathematicians might have realized that single-point errors do not paint the best picture in relation to the performance of the model on the whole dataset. We hence compute the average of all the errors; thereby getting 1/m∆©(y'-y)^2. 

Going back to the cost function (1/2m‚àë(y‚Ä≤‚àíy)^2) one might notice a ¬Ω at the beginning. This is something which is often used by mathematicians for simplification purposes when optimizing. It should be noted that when you take derivatives of the squared error function then that results in pulling down a factor equal to two from this exponent. Including ¬Ω cancels out this factor hence making it easier in working out derivatives.


**GRADIENT DESCENT ALGORITHM**

Having explored how errors are calculated, our next step is to figure out how to minimize these errors to improve the accuracy of our predictions.

For those of you familiar with calculus, you know it involves finding the minimum value of a function. Sometimes, we know the derivative of a function directly, but often, especially with more complex functions, the derivative isn't clear, and we have to apply numerical methods to find this minimum. This scenario is typical when using computers, as they cannot replicate the manual process of calculus to find minima. Those of you who've taken a numerical analysis course might already be familiar with this technique known as the Gradient Descent Algorithm.
Here‚Äôs how Gradient Descent works for a simple single-variable function, f(x): 
x_{n}=x_{n‚àí1}‚àíŒ±√óf‚Ä≤(x_{n‚àí1}) 

This formula is designed based on the understanding that the farther you are from the minimum, the steeper the slope. If the slope is negative (indicating you are left of the minimum), you should move right. And, if you are far from the minimum, you take larger steps; if you are near, you take smaller steps. The presence of the derivative in the formula adjusts your step automatically‚Äîlarge negative slopes push you significantly forward, and positive slopes pull you back.

We can adjust the Œ± value, often called the learning rate, to control how quickly we approach this minimum. Once the derivative (the slope) nears zero, indicating proximity to the minimum, we can set a threshold to stop further iterations if the magnitude of f‚Ä≤(x) is sufficiently small.

**MULTIVARIABLE GRADIENT DESCENT**

In the previous section, we looked at gradient descent for a single-variable function. However, in machine learning, we deal with multiple parameters simultaneously, which leads us to **multivariable gradient descent**.

In multivariable gradient descent, the cost function J(\theta_0, \theta_1, \theta_2, ..., \theta_n) depends on multiple parameters ( \theta_0, \theta_1, \theta_2, ..., \theta_n \). The goal remains the same: minimize this cost function by iteratively adjusting each parameter.

The generalized formula for updating each parameter is:

\[
\theta_{i} = \theta_{i} - \alpha \frac{\partial}{\partial \theta_{i}} J(\theta_0, \theta_1, \theta_2, ..., \theta_n)
\]

Where:
- \( \theta_{i} \) is the parameter being updated.
- \( \alpha \) is the learning rate.
- \( \frac{\partial}{\partial \theta_{i}} J(\theta_0, \theta_1, \theta_2, ..., \theta_n) \) is the partial derivative of the cost function with respect to \( \theta_{i} \), which indicates how much the cost function changes with a small change in \( \theta_{i} \).

**Example: Linear Regression with Gradient Descent**

To make this more concrete, consider the example of linear regression with two parameters: slope \( m \) and y-intercept \( b \). The hypothesis function is:

\[
h_\theta(x) = mx + b
\]

Here, \( m \) and \( b \) are the parameters to be optimized using gradient descent. The cost function for linear regression is:

\[
J(m, b) = \frac{1}{2m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2
\]

The goal is to minimize this cost function by updating \( m \) and \( b \) using gradient descent. The parameter updates are as follows:

\[
m = m - \alpha \frac{1}{m} \sum_{i=1}^{m} \left( (h_\theta(x^{(i)}) - y^{(i)}) x^{(i)} \right)
\]

\[
b = b - \alpha \frac{1}{m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right)
\]

This process is repeated until the cost function converges to a minimum.

**Why Does Gradient Descent Work?**

The core idea behind gradient descent is to use the slope (i.e., the derivative) of the cost function to determine the direction of steepest descent. If the slope is large and positive, gradient descent decreases the parameter value, and if it‚Äôs large and negative, it increases the parameter value.

The learning rate \( \alpha \) plays an essential role in controlling the size of the steps:
- A large \( \alpha \) results in larger steps, potentially overshooting the minimum.
- A small \( \alpha \) results in smaller steps, leading to slower but more precise convergence.

Gradient descent typically stops when the changes in the cost function are below a certain threshold or after a fixed number of iterations.

**Conclusion**

Gradient descent is a powerful optimization tool, fundamental to machine learning. Its ability to handle multiple parameters simultaneously makes it ideal for complex models. However, it is crucial to choose the right learning rate and apply feature scaling for efficient convergence.
